{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6a8bda-fa84-4846-9f07-c426f6488c79",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "* **Products used:** \n",
    "[dem_cop_30](https://explorer.digitalearth.africa/products/s2_l2a), [s2_l2a](https://explorer.digitalearth.africa/products/dem_cop_90), [dem_srtm](https://explorer.digitalearth.africa/products/dem_srtm), [dem_srtm_deriv](https://explorer.digitalearth.africa/products/dem_srtm_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d049a6-588a-430b-96a6-6233cc208642",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "Training data extraction plays a crucial role in training machine learning models. The process involves extracting relevant feature layers from a geospatial dataset based on predefined geometries or regions of interest. This enables the creation of accurate and reliable classification models for various applications such as land cover mapping, crop monitoring, and environmental analysis.\n",
    "\n",
    "To facilitate this task, the open-data-cube provides a powerful function called \"collect_training_data.\" This function is part of the deafrica_tools.classification script and is specifically designed to extract training data from the open-data-cube using geometries defined within a GeoJSON file. The GeoJSON file contains the spatial boundaries or polygons that delineate the regions of interest for which training data needs to be extracted.\n",
    "\n",
    "## Description:\n",
    "\n",
    "This notebook focuses on the extraction of training data (feature layers) from the open-data-cube using geometries defined within a GeoJSON file. It follows a step-by-step approach to guide users in utilizing the \"collect_training_data\" function effectively. The goal is to enable users to extract the appropriate training data for their specific use case.\n",
    "\n",
    "The main steps in this notebook are as follows:\n",
    "\n",
    "1. **Previewing the Training Data:** The notebook starts by plotting the polygons from the training data on a basemap. This visualization provides users with a visual representation of the regions of interest for which training data will be extracted.\n",
    "\n",
    "2. **Defining the Feature Layer Function:** Next, a feature layer function is defined. This function specifies the set of feature layers to be extracted from the open-data-cube. These layers are carefully selected based on their relevance to the classification task at hand.\n",
    "\n",
    "3. **Extracting Training Data:** The \"collect_training_data\" function is then employed to extract the training data from the datacube. It utilizes the predefined geometries from the GeoJSON file and retrieves the corresponding feature layers. This step ensures that the extracted data aligns precisely with the defined regions of interest.\n",
    "\n",
    "4. **Exporting Training Data:** Finally, the extracted training data is exported and saved to disk. This facilitates its subsequent use in other scripts or machine learning workflows for training classification models.\n",
    "\n",
    "By following the steps outlined in this notebook, users can leverage the \"collect_training_data\" function to efficiently extract training data from the open-data-cube. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f89ea0-5373-455e-a908-8d3233b535ba",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f926deb-9a6f-4ad8-a808-fa1abf7ba1bb",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e9a94-c117-45f6-a4d7-93025def990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import io\n",
    "import os \n",
    "# import math\n",
    "# import datacube\n",
    "# import warnings\n",
    "# import rioxarray\n",
    "# import rasterio\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "#import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import rioxarray as rx\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# from datacube.testutils.io import rio_slurp_xarray\n",
    "# from deafrica_tools.datahandling import load_ard\n",
    "# from deafrica_tools.plotting import map_shapefile\n",
    "# from deafrica_tools.bandindices import calculate_indices\n",
    "#from classification import collect_training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072cdfd-2116-4354-b1eb-8875a12aa930",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    " * path: The path to the input vector file from which we will extract training data. A default geojson is provided.\n",
    " * field: This is the name of column in your shapefile attribute table that contains the class labels. The class labels must be integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c624d2a-c9ed-4c4b-be9c-d011f424afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a prefix to identify the area of interest in the saved outputs\n",
    "# By assigning the desired prefix, you can easily identify the outputs associated with the specific area of interest.\n",
    "SKYKOMISH_HUC8_ID = \"17110009\"\n",
    "prefix = SKYKOMISH_HUC8_ID\n",
    "field = \"class_id\"\n",
    "path = f\"data/{prefix}_training_samples.geojson\"\n",
    "# swap out to wherever upstream processes are saving the rasters\n",
    "ras_dir = \"C:/Users/Arianna/Documents/rf_testing/rasters\"\n",
    "\n",
    "# Load input data shapefile\n",
    "training_points = gpd.read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ff005-92c1-4151-a413-65e216412d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set a flag to convert to polygons:\n",
    "# use_polygons = False\n",
    "\n",
    "# if use_polygons:\n",
    "#     # Convert from lat,lon to EPSG:6933 (projection in metres)\n",
    "#     training_points = training_points.to_crs(\"EPSG:6933\")\n",
    "\n",
    "#     # Buffer geometry to get a square - only if trying to sample multiple pixels\n",
    "#     buffer_radius_m = 10\n",
    "#     training_points.geometry = training_points.geometry.buffer(\n",
    "#         buffer_radius_m, cap_style=3\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b4aac-b5ad-47f8-a668-f8d44322310b",
   "metadata": {},
   "source": [
    "#### Plot on interactive map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac8d79-e930-4823-8f21-7219b78ffcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points = training_points\n",
    "# points.explore(\n",
    "#     tiles=\"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
    "#     attr=\"Imagery @2022 Landsat/Copernicus, Map data @2022 Google\",\n",
    "#     popup=True,\n",
    "#     cmap=\"viridis\",\n",
    "#     style_kwds=dict(radius=5, color=\"red\", fillOpacity=0.8, fillColor=\"red\", weight=3),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c40e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem_file = \"../../../data/processed/17110009_Skykomish_HE_DEM_3m.tif\"\n",
    "# CRS = rx.open_rasterio(ras_paths[0], chunks=True).squeeze(drop=True).rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32eb975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the raster outputs of the TWI scripts and Dan's makegrids ended up\n",
    "# with origins offset 0.5m from each other in the x and y directions.\n",
    "# when this is true, load_training_data will yield errors and the sampling\n",
    "# in the next cell will get messed up WITHOUT showing any errors\n",
    "def load_training_data():\n",
    "    return xr.merge(\n",
    "        [\n",
    "            rx.open_rasterio(f, chunks=True).squeeze(drop=True).to_dataset(name=f.stem)\n",
    "            for f in Path(ras_dir).glob(\"*.tif\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "training_xr = load_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ed2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_xr(xr_ds: xr.Dataset, points: gpd.GeoDataFrame):\n",
    "    points_proj = training_points\n",
    "    pts_da = points_proj.assign(\n",
    "        x=points_proj.geometry.x, y=points_proj.geometry.y\n",
    "    ).to_xarray()\n",
    "\n",
    "    # a dataframe or series (for a single point)\n",
    "    pt_values_i = (\n",
    "        xr_ds.sel(pts_da[[\"x\", \"y\"]], method=\"nearest\").squeeze().compute().to_pandas()\n",
    "    )\n",
    "\n",
    "    if isinstance(pt_values_i, pd.Series):\n",
    "        pt_values_i = pt_values_i.to_frame().transpose()\n",
    "        pt_values_i.index = points.index\n",
    "\n",
    "    return pd.concat([points_proj, pt_values_i], axis=1)\n",
    "\n",
    "pd_training_features = sample_xr(training_xr, training_points)\n",
    "pd_training_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58724be-7158-420d-b6ac-375db82c11f4",
   "metadata": {},
   "source": [
    "### Export training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfb224-c1c8-4393-ae6f-6686e2a75d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the name and location of the output file\n",
    "# output_file = \"results/training_features.txt\"\n",
    "output_file = f\"data/{prefix}_training_features.csv\"\n",
    "# Export files to disk\n",
    "pd_training_features.to_csv(output_file, header=True, index=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c058b89-0556-40a2-99d5-3beb3120a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geopandas dataframe\n",
    "gpd_training_features = gpd.GeoDataFrame(\n",
    "    pd_training_features,\n",
    "    geometry=\"geometry\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d67d1-0a31-41a7-80c5-54e7e51087dd",
   "metadata": {},
   "source": [
    "#####  Add a column for binary (wetland/non-wetland) classification\n",
    "This block ensures that both binary and multi-class classification labels are properly set up from the original `class_id` field:\n",
    "\n",
    "- If `class_id` contains only two values (0 and 1), it is assumed to be binary, and is renamed to `class_id_binary`.\n",
    "- If `class_id` includes additional wetland types, a new binary column `class_id_binary` is created:\n",
    "  - `1` for any wetland type (i.e., values not equal to 0)\n",
    "  - `0` for non-wetland (i.e., value equal to 0)\n",
    "  - The original column is then renamed to `class_id_type` for use in multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ec785-2a4c-4d9e-ad91-c7ec1c7b9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if unique values in 'class_id' are only 0 and 1\n",
    "unique_values = gpd_training_features[\"class_id\"].unique()\n",
    "if len(unique_values) == 2 and set(unique_values) == {0, 1}:\n",
    "    # Replace 'class_id' with 'class_id_binary'\n",
    "    gpd_training_features.rename(columns={\"class_id\": \"class_id_binary\"}, inplace=True)\n",
    "else:\n",
    "    # Create 'class_id_binary' column based on condition\n",
    "    gpd_training_features[\"class_id_binary\"] = gpd_training_features[\"class_id\"].apply(\n",
    "        lambda x: 1 if x != 0 else 0\n",
    "    )\n",
    "    gpd_training_features.rename(columns={\"class_id\": \"class_id_type\"}, inplace=True)\n",
    "\n",
    "# Insert the new column at the second position\n",
    "gpd_training_features.insert(\n",
    "    0, \"class_id_binary\", gpd_training_features.pop(\"class_id_binary\")\n",
    ")\n",
    "print(gpd_training_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b22f43-2dba-4d96-a085-23a8ee428338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-zero values in the 'class_id' column with 1\n",
    "gpd_training_features[\"class_id_binary\"] = gpd_training_features[\"class_id_type\"].apply(\n",
    "    lambda x: 1 if x != 0 else 0\n",
    ")\n",
    "# Insert the new column at the second position\n",
    "gpd_training_features.insert(\n",
    "    1, \"class_id_binary\", gpd_training_features.pop(\"class_id_binary\")\n",
    ")\n",
    "gpd_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2b5d6-d1bf-4924-b99e-00a59d60fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as geojson file\n",
    "results_dir = Path(\"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "geojson_file = f\"results/{prefix}_training_features.geojson\"\n",
    "gpd_training_features.to_file(geojson_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5532548-5fa3-4e7c-8f74-fc6b3087b006",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897d749-5f90-4287-a9b2-5a1f1c708008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
