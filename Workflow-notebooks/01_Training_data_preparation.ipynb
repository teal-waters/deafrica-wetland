{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5de31c8-068a-472f-9ff6-64a8d514adcb",
   "metadata": {},
   "source": [
    "# Training data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67447f89-60df-4139-9896-74cb4f9ae52a",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In machine learning, training data plays a crucial role in training algorithms or models to accurately predict specific outcomes or answers. When it comes to wetland classification, ensuring that the training dataset includes a diverse representation of different wetland types is essential. This is where stratified random sampling comes into play.\n",
    "\n",
    "Stratified random sampling is a method that takes into account the variations and diversity within wetland areas. By stratifying the sampling process, the aim is to ensure that each wetland type, such as marshes, swamps, or bogs, is proportionately included in the generated samples. This approach is important because it helps to capture the distinct characteristics and patterns associated with each wetland type.\n",
    "\n",
    "By including representative samples from various wetland types, the resulting wetland classification model becomes more robust and accurate in identifying and classifying different wetland categories. It enables the model to learn and understand the specific features, such as vegetation composition, hydrological characteristics, or soil properties, that define each wetland type.\n",
    "\n",
    "Additionally, the balanced representation of wetland types in the training dataset helps to mitigate the risk of biases or disproportionate emphasis on certain wetland categories. This ensures a more comprehensive understanding of the overall wetland ecosystem and enhances the model's ability to generalize its classification predictions to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fe71b-4d41-4e8b-9cf7-db9f69d2e9e5",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The purpose of this notebook is to generate samples for training a wetland classification model. The notebook focuses on creating representative samples of both wetland and non-wetland areas in order to train an accurate and robust model.\n",
    "\n",
    "The following steps outline the process:\n",
    "\n",
    "1. **Load the study area and wetland inventory vectors:** In this step, the study area boundaries and the vector data representing wetland locations are loaded.\n",
    "\n",
    "2. **Sample wetland and non-wetland areas:** The notebook employs a sampling strategy based on the specific scenario:\n",
    "\n",
    "    a. **Sampling with wetland types:** If there are multiple wetland types present, the notebook utilizes stratified random sampling. This approach\n",
    "    proportionately includes samples from each wetland type, capturing the variations and diversity within wetland areas. By considering the\n",
    "    distinct characteristics of each wetland type, the wetland classification model can accurately classify different wetland categories.\n",
    "\n",
    "    b. **Sampling without wetland types:** When there is only one wetland type or no wetland types present, random sampling is used. This\n",
    "    sampling approach ensures a balanced spatial distribution across the study area.\n",
    "    \n",
    "    c. **Non-Wetland area:** For non-wetland areas, random sampling is utilised to ensure a balanced spatial distribution across the study area.This means that samples are randomly selected from the available non-wetland areas, taking into account the proportion of non-wetland areas to the total study area. The purpose of this sampling approach is to capture the spatial patterns and characteristics specific to the non-wetland class.\n",
    "    \n",
    "\n",
    "3. **Create Combine the wetland and non-wetland samples:** The wetland and non-wetland samples generated from the previous step are combined into a single dataset. This combined dataset will be used for training the wetland classification model. By including both wetland and non-wetland samples, the model can learn to distinguish and classify different types of wetlands accurately.\n",
    "\n",
    "By following these steps, the notebook creates a comprehensive and balanced training dataset for the wetland classification model, enabling it to accurately identify and classify wetland areas based on their distinct characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cff119-edb1-4b74-bf60-c258fa410bad",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2aa274-f556-4ab4-a08d-f6bd3f018868",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028afc20-cb66-4037-a5d8-10427aedcd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from odc.geo.geom import Geometry\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from shapely import speedups\n",
    "#from shapely.ops import triangulate\n",
    "#from shapely.strtree import STRtree\n",
    "from shapely.geometry import Point#, Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from deafrica_tools.plotting import display_map, map_shapefile, plot_lulc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b13fb",
   "metadata": {},
   "source": [
    "## Some constants for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKYKOMISH_HUC8_ID = \"17110009\"\n",
    "target_crs = \"epsg:6350\"\n",
    "# Wetland class column name\n",
    "wetland_class_column = \"WETLAND_TY\"  # Modify this with your actual column name\n",
    "wetland_subclass_column = \"ATTRIBUTE\"\n",
    "\n",
    "# Exclusions categories - talk with Meghan about these.\n",
    "# she says another group excluded these categories\n",
    "# see https://www.fws.gov/sites/default/files/documents/wetlands-and-deepwater-map-code-diagram.pdf\n",
    "# for an explanation of NWI codes\n",
    "exclusions =[ # you can comment out lines that you DO want to include\n",
    "    \"R1\", # Riverine tidal\n",
    "    \"R2\", # Riverine lower perennial\n",
    "    \"R3\", # Riverine upper perennial\n",
    "    \"R4\", # Riverine intermittent\n",
    "    \"R5\", # Riverine other\n",
    "    \"L1\", # Lacustrine limnetic\n",
    "    \"L2\"  # Lacustrine littoral\n",
    "]\n",
    "\n",
    "# buffer distance - number specifying minimum distance from\n",
    "# the edge of an NWI polygon a sample can be. Low positional accuracy on NWI\n",
    "# means that samples near polygon edges might be miscategorized\n",
    "buffer_dist = 20\n",
    "\n",
    "# Define the total number of points to sample\n",
    "total_pts_wet = 2000 # number of wetland points to sample\n",
    "min_points = 300 # minimum number of points per wetland class\n",
    "total_pts_bg = total_pts_wet # number of background points to sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffab535-25cf-4f38-af16-1464efc2aacb",
   "metadata": {},
   "source": [
    "## Load the study area and wetland inventory vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f3a01-48b2-433e-9116-52770bd939b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_huc_boundary(huc_id) -> gpd.GeoDataFrame:\n",
    "    url = \"https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer/4/query\"\n",
    "    params = {\n",
    "        \"where\": f\"HUC8='{huc_id}'\",\n",
    "        \"outFields\": \"*\",\n",
    "        \"returnGeometry\": \"true\",\n",
    "        \"f\": \"geojson\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "aoi = load_huc_boundary(SKYKOMISH_HUC8_ID)\n",
    "\n",
    "aoi_geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "aoi_geopolygon_gdf = gpd.GeoDataFrame(geometry=[aoi_geopolygon], crs=aoi_geopolygon.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load area of interest vector\n",
    "\n",
    "def get_nwi(huc8_id: str):\n",
    "    return gpd.read_file(\n",
    "        f\"/vsizip//vsicurl/https://documentst.ecosphere.fws.gov/wetlands/downloads/watershed/HU8_{huc8_id}_Watershed.zip/HU8_{huc8_id}_Watershed/HU8_{huc8_id}_Wetlands.shp\"\n",
    "    )\n",
    "\n",
    "\n",
    "prefix = SKYKOMISH_HUC8_ID\n",
    "nwi = get_nwi(SKYKOMISH_HUC8_ID)\n",
    "nwi.to_file(f\"data/nwi_{SKYKOMISH_HUC8_ID}.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wetland polygons, do some dissolving and buffering\n",
    "wetland_inventory_gdf = gpd.overlay(\n",
    "    nwi.to_crs(target_crs), aoi_geopolygon_gdf.to_crs(target_crs), how=\"intersection\"\n",
    ")\n",
    "\n",
    "# we can run buffer operations on the *dissolved* polygons to remove areas\n",
    "# at the edges of wetland complexes while retaining internal areas\n",
    "wetland_inventory_full = wetland_inventory_gdf.dissolve()\n",
    "# negative and positive buffers\n",
    "wetland_inventory_interior = wetland_inventory_full.buffer(-buffer_dist).to_frame() # presence samples must fall INSIDE these polygons\n",
    "wetland_inventory_exterior = wetland_inventory_full.buffer(buffer_dist).to_frame() # background samples must fall OUTSIDE these larger polys\n",
    "\n",
    "wetland_inventory_filt = gpd.overlay(\n",
    "    wetland_inventory_gdf[~wetland_inventory_gdf[wetland_subclass_column].str.startswith(tuple(exclusions))],\n",
    "    wetland_inventory_interior)\n",
    "\n",
    "# Plot the area of interest (AOI) and the Wetland polygons\n",
    "fig, ax = plt.subplots()\n",
    "base = aoi_geopolygon_gdf.to_crs(target_crs).plot(ax=ax, color=\"white\", edgecolor=\"black\")\n",
    "wetland_inventory_filt.plot(ax=base, marker=\"o\", color=\"blue\", markersize=5)\n",
    "legend_patch = mpatches.Patch(color=\"blue\", label=\"Wetland\")\n",
    "plt.title(f\"{SKYKOMISH_HUC8_ID} wetlands\")\n",
    "ax.legend(handles=[legend_patch], loc=\"upper right\")\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f61213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the following wetland categories\n",
    "\n",
    "# Riverine tidal R1...\n",
    "# Riverine lower perennial R2...\n",
    "# Riverine upper perennial R3...\n",
    "# Riverine unknown\n",
    "# Lacustrine limnetic (not sure) L1...\n",
    "# Lacustrine (not sure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58708a-9aef-4322-81f5-a6816d4dac52",
   "metadata": {},
   "source": [
    "## Sample training data \n",
    "\n",
    "Sampling wetland points for training data can be approached in two ways, depending on the available data. The two options are:\n",
    "\n",
    "1. **Sampling with wetland types:** If the wetland data includes information about different types of wetlands (e.g., marshes, swamps, bogs), you can perform sampling while considering the specific wetland types. In this approach, the sampling process takes into account the different wetland types present in the data and aims to obtain representative samples from each wetland type. By ensuring the inclusion of samples from each wetland type, the resulting training data can better capture the characteristics and patterns associated with each wetland type.\n",
    "\n",
    "2. **Sampling without wetland types:** In cases where the wetland data does not provide information about different wetland types or if wetland types are not applicable to the dataset, the sampling process focuses solely on wetland areas as a whole. This approach disregards the specific wetland types and aims to obtain representative samples from all wetland areas collectively. The goal is to capture the overall characteristics and patterns exhibited by wetland areas, without considering the differentiation between specific wetland types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1e111-17c5-43a4-badd-68b13455f03e",
   "metadata": {},
   "source": [
    "### Sampling with wetland types\n",
    "This code samples points within polygons based on a wetland inventory GeoDataFrame. It starts by defining the total number of points to sample. The wetland polygons are then grouped by class, which means that polygons with the same wetland class are grouped together. To ensure a proportional sampling approach, the code calculates the total area of each wetland class by summing the areas of the polygons within each class. This information is then used to determine the proportion of the total area that each wetland class represents. An empty list is created to store the sampled points. The code proceeds to sample points from each wetland class based on the proportion of the total area it represents. It does this by randomly selecting a polygon from each class and then sampling a random point within the bounding box of that polygon. The code checks if the sampled point falls within the selected polygon. If it does, the point is added to the list of sampled points. If it doesn't, it tries again. This process continues until the desired number of points for each wetland class is reached. In cases where there are more points sampled than desired, the code removes the excess points to match the specified total number of points. Conversely, if there are not enough points sampled, the code randomly samples points from all polygons until the desired number is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd12d64-8c3d-4125-a654-127e19a58463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first five rows of the wetlands geodatframe to identify the column with the wetland classes\n",
    "wetland_inventory_filt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afed12-17b6-49bc-9915-79b21cd61113",
   "metadata": {},
   "source": [
    "***Make sure to change the wetland_class_column to the appropriate name in your data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f7de0-e375-48ac-bc7e-17a9122380a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique classes within the wetland class column\n",
    "unique_classes = wetland_inventory_filt[wetland_class_column].unique()\n",
    "\n",
    "# Assign a numerical ID to each unique class\n",
    "class_id_mapping = {\n",
    "    class_name: class_id for class_id, class_name in enumerate(unique_classes, start=1)\n",
    "}\n",
    "\n",
    "# Group polygons by the wetland class column\n",
    "grouped = wetland_inventory_filt.groupby(wetland_class_column)\n",
    "\n",
    "# Calculate the total area of each stratum\n",
    "strata_areas = grouped.apply(lambda x: x.area.sum())\n",
    "\n",
    "# Calculate the proportion of the total area that each stratum represents\n",
    "strata_props = strata_areas / strata_areas.sum()\n",
    "\n",
    "# Calculate the weight factor based on the size of each polygon\n",
    "strata_weights = strata_areas / strata_areas.max()\n",
    "\n",
    "# Create an empty list to store the sampled points\n",
    "wetland_points = []\n",
    "\n",
    "# Create a dictionary to store the number of points sampled for each class\n",
    "class_points_dict = {}\n",
    "\n",
    "# Sample points from each stratum\n",
    "for class_name, group in tqdm(grouped, total=len(grouped), desc=\"Sampling progress\"):\n",
    "    # Get the numerical ID for the current class\n",
    "    class_id = class_id_mapping[class_name]\n",
    "\n",
    "    # Calculate the number of points to sample for this stratum\n",
    "    num_points = np.ceil(strata_props[class_name] * total_pts_wet).astype(int)\n",
    "\n",
    "    # Check if the number of points to sample is less than a desired amount (e.g. 30 points)\n",
    "    if num_points < min_points:\n",
    "        num_points = min_points\n",
    "\n",
    "    # Calculate the weight for this stratum based on the polygon's size\n",
    "    weight = strata_weights[class_name]\n",
    "\n",
    "    # Initialize a counter to keep track of the number of points sampled for this stratum\n",
    "    points_sampled = 0\n",
    "\n",
    "    # Repeat until we have sampled the desired number of points for this stratum\n",
    "    while points_sampled < num_points:\n",
    "        # Sample a random polygon from this stratum, considering the weight\n",
    "        polygon = (\n",
    "            group.sample(n=1, weights=group.geometry.area * weight).iloc[0].geometry\n",
    "        )\n",
    "\n",
    "        # check that point is within polygon. If not, try again.\n",
    "        # this reduces bias against narrow features that are oriented diagonally\n",
    "        # wrt the coordinate system\n",
    "        found_valid_point = False\n",
    "        attempts = 0\n",
    "        while not found_valid_point:\n",
    "            # Sample a random point within the bounding box of the polygon\n",
    "            x_min, y_min, x_max, y_max = polygon.bounds\n",
    "            x = np.random.uniform(x_min, x_max)\n",
    "            y = np.random.uniform(y_min, y_max)\n",
    "\n",
    "            if polygon.contains(Point(x, y)):\n",
    "                wetland_points.append(((x, y), class_id, class_name))\n",
    "                points_sampled += 1\n",
    "                found_valid_point = True\n",
    "\n",
    "    # If we have more points than desired, remove the excess\n",
    "    if len(wetland_points) > total_pts_wet:\n",
    "        wetland_points = random.sample(wetland_points, total_pts_wet)\n",
    "\n",
    "    # Store the number of points sampled for this class\n",
    "    class_points_dict[class_name] = points_sampled\n",
    "\n",
    "# Convert the points list to a GeoDataFrame\n",
    "wetland_samples_gdf = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(\n",
    "        [p[0][0] for p in wetland_points], [p[0][1] for p in wetland_points]\n",
    "    ),\n",
    "    data={\n",
    "        \"class_id\": [p[1] for p in wetland_points],\n",
    "        \"class_name\": [p[2] for p in wetland_points],\n",
    "    },  # Add class_id and class_name as columns\n",
    ")\n",
    "\n",
    "# Plot the wetland polygons and the sampled points\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "wetland_inventory_filt.plot(ax=ax, alpha=0.5)\n",
    "wetland_samples_gdf.plot(\n",
    "    ax=ax, column=\"class_name\", categorical=True, legend=True, cmap=\"Set1\", markersize=5\n",
    ")\n",
    "plt.title(f\"{prefix} wetland samples\")\n",
    "plt.show()\n",
    "\n",
    "# Display the number of sampled points for each class after removing the excess\n",
    "for class_name, num_points in class_points_dict.items():\n",
    "    if class_name in wetland_samples_gdf[\"class_name\"].unique():\n",
    "        num_points_after_removal = (\n",
    "            wetland_samples_gdf[\"class_name\"] == class_name\n",
    "        ).sum()\n",
    "        print(f\"Number of points sampled for {class_name}: {num_points_after_removal}\")\n",
    "    else:\n",
    "        print(f\"Number of points sampled for {class_name}: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88af6a-b994-4788-82a7-5ce1175e4cb9",
   "metadata": {},
   "source": [
    "### Class balancing \n",
    "A challenge encountered during the labeling of training data is class imbalance, which refers to the uneven distribution of classes within the dataset. During model training, the algorithm aims to optimize its performance over all training data. Therefore, an imbalanced training dataset can lead to a biased model. For example, when faced with a minority class, the model is more likely to misclassify it as a majority class rather than the other way around. In some cases, the distribution of the training data is intentionally designed to mirror the true distribution of the classes, and the resulting predictions are expected to be biased against the infrequent class. Alternatively, accuracy requirements may differ for various classes, making it desirable to adjust the proportions of the training labels. This adjustment ensures that there are more training data points available for classes where commission error (false positive) is preferred over omission error (false negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f474071-1150-473b-9a66-b51676d0db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique class names from \"class_name\" column\n",
    "class_names = wetland_samples_gdf[\"class_name\"].unique()\n",
    "\n",
    "# Count the number of samples for each wetland class\n",
    "class_counts = wetland_samples_gdf[\"class_name\"].value_counts()\n",
    "\n",
    "# Plot training samples distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(class_counts.index, class_counts.values)\n",
    "plt.xlabel(\"Wetland Class\")\n",
    "plt.ylabel(\"Number of Training Samples\")\n",
    "plt.title(\"Training Samples Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20aecdf-63b3-4d28-95c3-e7487e14db12",
   "metadata": {},
   "source": [
    "We then reduce the number of samples for classes with significantly larger sample size than the other classes. In this case, we will allow sample size within 10 times the smallest sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a1d6a-defe-4152-ac6e-c551d4c69fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = class_counts.min()\n",
    "for class_name in class_counts.index:\n",
    "    class_count = class_counts[class_name]\n",
    "    if class_count > 10 * min_count:\n",
    "        n_samples_dropped = class_count - 10 * min_count\n",
    "        if n_samples_dropped > class_count:\n",
    "            n_samples_dropped = class_count\n",
    "        if n_samples_dropped >= class_count:\n",
    "            print(\"No balancing needed for {}\".format(class_name))\n",
    "        else:\n",
    "            print(\"Dropping {} of {} samples\".format(n_samples_dropped, class_name))\n",
    "            wetland_samples_gdf.drop(\n",
    "                wetland_samples_gdf[wetland_samples_gdf[\"class_name\"] == class_name]\n",
    "                .sample(n=n_samples_dropped, replace=False)\n",
    "                .index,\n",
    "                axis=0,\n",
    "                inplace=True,\n",
    "            )\n",
    "    else:\n",
    "        print(\"No balancing needed\")\n",
    "\n",
    "# Update class_counts after balancing\n",
    "class_counts = wetland_samples_gdf[\"class_name\"].value_counts()\n",
    "\n",
    "# Get class names\n",
    "class_legends = [class_name for class_name in class_counts.index]\n",
    "\n",
    "# Plot training samples distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(class_legends, class_counts.values)\n",
    "plt.xlabel(\"Wetland Class\")\n",
    "plt.ylabel(\"Number of Training Samples\")\n",
    "plt.title(\"Wetland Type Samples Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88595810-035b-4349-991c-045b5bbd75e3",
   "metadata": {},
   "source": [
    "### (Alternative) Sampling without wetland types\n",
    "This code samples points uniformly within each individual wetland polygon, without considering the wetland class. It iterates over each polygon in the GeoDataFrame, samples a specified number of points (10 by default) uniformly within the bounding box of each polygon, and checks if the sampled points fall within the polygon. It repeats this process until the desired number of points is reached for each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeabe5-3830-4bc7-b312-5487bcaad555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the sampled points\n",
    "# wetland_points = []\n",
    "# total_points = 250\n",
    "\n",
    "# # Iterate over all wetland polygons\n",
    "# for polygon in tqdm(wetland_inventory_gdf.geometry,\n",
    "#                     total=len(wetland_inventory_gdf),\n",
    "#                     desc=\"Sampling progress\"):\n",
    "#     num_points = int(total_points / len(wetland_inventory_gdf))\n",
    "#     points_sampled = 0\n",
    "\n",
    "#     # Repeat until we have sampled the desired number of points for this polygon\n",
    "#     while points_sampled < num_points:\n",
    "#         # Sample a random point within the bounding box of the polygon\n",
    "#         x_min, y_min, x_max, y_max = polygon.bounds\n",
    "#         x = random.uniform(x_min, x_max)\n",
    "#         y = random.uniform(y_min, y_max)\n",
    "#         point = (x, y)\n",
    "\n",
    "#         # Check if the point is within the polygon\n",
    "#         if polygon.contains(Point(point)):\n",
    "#             # Append the point coordinates\n",
    "#             wetland_points.append(point)\n",
    "#             points_sampled += 1\n",
    "\n",
    "# # If the desired number of total points is not reached yet\n",
    "# while len(wetland_points) < total_points:\n",
    "#     # Randomly select a wetland polygon\n",
    "#     polygon = random.choice(wetland_inventory_gdf.geometry)\n",
    "\n",
    "#     # Sample a random point within the bounding box of the polygon\n",
    "#     x_min, y_min, x_max, y_max = polygon.bounds\n",
    "#     x = random.uniform(x_min, x_max)\n",
    "#     y = random.uniform(y_min, y_max)\n",
    "#     point = (x, y)\n",
    "\n",
    "#     # Check if the point is within the polygon\n",
    "#     if polygon.contains(Point(point)):\n",
    "#         # Append the point coordinates\n",
    "#         wetland_points.append(point)\n",
    "\n",
    "# # Print the number of points sampled\n",
    "# print(f\"Number of points sampled: {len(wetland_points)}\")\n",
    "\n",
    "# # Convert the points list to a GeoDataFrame\n",
    "# wetland_samples_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(\n",
    "#     [p[0] for p in wetland_points], [p[1] for p in wetland_points]))\n",
    "\n",
    "# # Add an ID column with a value of 1 to the GeoDataFrame\n",
    "# wetland_samples_gdf[\"class_id\"] = 1\n",
    "# # Add a class name column with the value \"wetland\"\n",
    "# wetland_samples_gdf[\"class_name\"] = \"Wetland\"\n",
    "\n",
    "# # Plot the wetland polygons and the sampled points\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# wetland_inventory_gdf.plot(ax=ax, alpha=0.5)\n",
    "# wetland_samples_gdf.plot(ax=ax, color='Blue', markersize=5)\n",
    "# plt.title(\"Randomly Sampled Points within Wetland Areas\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5130cd9-acce-4395-944b-00185e979c55",
   "metadata": {},
   "source": [
    "### Non-wetland sampling\n",
    "\n",
    "This code generates a set number of random points (specified by total_pts_ng) from within a single polygon representing non-wetland areas. It does this by repeatedly generating random points within the bounds of the polygon until the desired number of points is reached. The code checks each point to ensure that it is actually within the polygon, and keeps track of the number of points sampled using a progress bar. Finally, it prints the total number of points generated. For filtered sampling we will use the *full* wetland inventory, so excluded wetland categories will not explicitly appear in either the presence or background training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb186-2cc1-47e9-ae76-31d76a2986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points from the AOI using simple random sampling\n",
    "bounds = aoi_geopolygon_gdf.to_crs(target_crs).total_bounds\n",
    "\n",
    "non_wetland_points = []\n",
    "with tqdm(total=total_pts_bg) as pbar:  # set up the progress bar\n",
    "    while len(non_wetland_points) < total_pts_bg:\n",
    "        x = random.uniform(bounds[0], bounds[2])\n",
    "        y = random.uniform(bounds[1], bounds[3])\n",
    "        point = (x, y)\n",
    "\n",
    "        # Check if the point is within the AOI and not in any wetland area\n",
    "        if aoi_geopolygon_gdf.to_crs(nwi.crs).geometry[0].contains(\n",
    "            Point(point)\n",
    "        ) and not any(wetland_inventory_exterior.geometry.contains(Point(point))):\n",
    "            non_wetland_points.append(point)\n",
    "            pbar.update(1)  # update the progress bar\n",
    "\n",
    "# Convert the points list to a GeoDataFrame\n",
    "non_wetland_samples_gdf = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(\n",
    "        [p[0] for p in non_wetland_points], [p[1] for p in non_wetland_points]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add an ID column with a value of 1 to the GeoDataFrame\n",
    "non_wetland_samples_gdf[\"class_id\"] = 0\n",
    "# Add a class name column with the value \"non-wetland\"\n",
    "non_wetland_samples_gdf[\"class_name\"] = \"Non-wetland\"\n",
    "\n",
    "# Plot the sampled points\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "aoi_geopolygon_gdf.to_crs(nwi.crs).plot(ax=ax, alpha=0.5)\n",
    "wetland_inventory_filt.plot(\n",
    "    ax=ax, marker=\"o\", color=\"blue\", markersize=5, label=\"Wetland\"\n",
    ")\n",
    "non_wetland_samples_gdf.plot(ax=ax, color=\"red\", markersize=5)\n",
    "plt.title(f\"{prefix} non-wetland samples\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total points sampled: {len(non_wetland_points)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d55f74-869c-40ef-a34e-81bac2be2a2d",
   "metadata": {},
   "source": [
    "## Combine wetland and non-wetland sample points\n",
    "\n",
    "The wetland and non-wetland samples generated from the previous step are combined into a single dataset. This combined dataset will be used for training the wetland classification model. By including both wetland and non-wetland samples, the model can learn to distinguish and classify different types of wetlands accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d4cde-a2ac-45c8-a72b-1541106106ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined GeoDataFrame with wetland and non-wetland samples\n",
    "combined_samples_gdf = pd.concat([wetland_samples_gdf, non_wetland_samples_gdf])\n",
    "combined_samples_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4f598-e676-4445-983f-ec12b7987f08",
   "metadata": {},
   "source": [
    "### Create a training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194344cf-2820-4831-9a66-a17097645c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80% training and 20% testing datasets\n",
    "train_df, test_df = train_test_split(\n",
    "    combined_samples_gdf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the shapes of the training and testing datasets\n",
    "print(\"Training dataset shape:\", train_df.shape)\n",
    "print(\"Testing dataset shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db64e7-f2ad-4263-b31d-be002179db32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the training samples GeoDataFrame as GeoJSON\n",
    "train_df.to_file(\n",
    "    f\"data/{prefix}_training_samples.geojson\",\n",
    "    driver=\"GeoJSON\",\n",
    "    crs=wetland_inventory_filt.crs,\n",
    "    engine=\"fiona\",\n",
    ")\n",
    "\n",
    "# Save the testing samples GeoDataFrame as GeoJSON\n",
    "test_df.to_file(\n",
    "    f\"data/{prefix}_testing_samples.geojson\",\n",
    "    driver=\"GeoJSON\",\n",
    "    crs=wetland_inventory_filt.crs,\n",
    "    engine=\"fiona\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cf5e7-c77f-41ab-b457-6d218563adba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export class label dictionary \n",
    "By exporting the class label dictionary, you can store and access the mapping between class IDs and class names in a structured format. This can be useful for reference and future use, such as when interpreting model predictions or evaluating performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde2857-f17d-467b-838f-d85f57162bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct ID and name combinations\n",
    "distinct_labels = combined_samples_gdf[[\"class_id\", \"class_name\"]].drop_duplicates()\n",
    "\n",
    "# Create label dictionary\n",
    "labels_dict = dict(zip(distinct_labels[\"class_name\"], distinct_labels[\"class_id\"]))\n",
    "\n",
    "# Print the label dictionary\n",
    "print(labels_dict)\n",
    "\n",
    "# Get distinct ID and name combinations\n",
    "distinct_labels = combined_samples_gdf[[\"class_id\", \"class_name\"]].drop_duplicates()\n",
    "\n",
    "# Create label dictionary\n",
    "labels_dict = dict(zip(distinct_labels[\"class_name\"], distinct_labels[\"class_id\"]))\n",
    "\n",
    "# Specify the file path where you want to save the JSON file\n",
    "file_path = f\"data/{prefix}_labels_dict.json\"\n",
    "\n",
    "# Export the labels_dict as a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(labels_dict, json_file)\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"The labels_dict has been exported as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95459d8a-3a64-4b7f-b855-de2e2217c208",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f27a21-a1e3-488f-a4c1-030e85498c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
